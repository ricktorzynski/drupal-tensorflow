<div>
  <p>The toxicity model detects whether text contains toxic content such as threatening language, insults, obscenities, identity-based hate, or sexually explicit language.</p>
  <p>Output is directed to console.log - use Inspect to view console.log.</p>
  <a href="https://github.com/tensorflow/tfjs-models/tree/master/toxicity">Toxicity Classifier on Github</a>
</div>

